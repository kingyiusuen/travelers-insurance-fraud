{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "In this file, I will perform data cleaning and feature engineering. \n",
    "\n",
    "We also considered using some macroeconomic data (such as unemployment rate, the price of S&P 500, etc), as we though that a bad economy might motivate more people to commit fraud, but since the prediction did not improve much, we did not include them in our final model and they won't be discussed further here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/raw/train.csv')\n",
    "test = pd.read_csv('../data/raw/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the observations whose the target variable `fraud` is equal to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['fraud'] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For values that match the following conditions, treat them as missing values to be imputed later.\n",
    "\n",
    "- `age_of_driver > 100`\n",
    "- `annual_income = -1`\n",
    "- `zip_code = -1`\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/List_of_the_verified_oldest_people), the oldest living person is 115, as of 2018. I think it is reasonable to assume that any `age_of_driver > 100` in this dataset is a clerical error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df.loc[df['age_of_driver'] > 100, 'age_of_driver'] = np.nan\n",
    "    df.loc[df['annual_income'] == -1, 'annual_income'] = np.nan\n",
    "    df.loc[df['zip_code'] == 0, 'zip_code'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do an imputation for the missing values. Since there is only a very small percentage of missing values, we will simply do a mean/mode imputation for the continuous/categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    # mean imputation for continuous variables\n",
    "    for feature in ['age_of_driver', 'annual_income', 'claim_est_payout', 'age_of_vehicle']:\n",
    "        feature_mean = df.loc[:, feature].mean(skipna=True)\n",
    "        df[feature].fillna(int(feature_mean), inplace=True)\n",
    "\n",
    "    # mode imputation for categorical variables\n",
    "    for feature in ['marital_status', 'witness_present_ind', 'zip_code']:\n",
    "        feature_mode = df.loc[:, feature].mode(dropna=True)\n",
    "        df[feature].fillna(feature_mode.values[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform `zip_code` into `latitude` and `longitude` using the data from [UnitedStatesZipCodes.org](https://www.unitedstateszipcodes.org/zip-code-database/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_code_database = pd.read_csv('../data/external/zip_code_database.csv')\n",
    "latitude_and_longitude_lookup = {\n",
    "    row.zip: (row.latitude, row.longitude) for row in zip_code_database.itertuples()\n",
    "}\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['latitude'] = df['zip_code'].apply(lambda x: latitude_and_longitude_lookup[x][0])\n",
    "    df['longitude'] = df['zip_code'].apply(lambda x: latitude_and_longitude_lookup[x][1])\n",
    "    df.drop(columns=['zip_code'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can't imagine how `claim_date` and `claim_day_of_week` are related to insurance fraud, so I decide to drop them to prevent adding noise to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train, test]:\n",
    "    df.drop(columns=['claim_date', 'claim_day_of_week'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/processed/train.csv', index=False)\n",
    "test.to_csv('../data/processed/test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03e93f2959c516196957ae17ec0aa5d1e9fc5dd82cbe13968d4cfc2a60558992"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
