{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This file shows how I performed data cleaning and feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_df = pd.read_csv(\"../data/raw/train.csv\")\n",
    "test_df = pd.read_csv(\"../data/raw/test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the test set provided does not have the target variable, so we have to create an internal validation set to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(full_train_df, test_size=0.2, stratify=full_train_df[\"fraud\"], random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the observations whose the target variable `fraud` is equal to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df[\"fraud\"] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For values that match the following conditions, treat them as missing values to be imputed later.\n",
    "\n",
    "- `age_of_driver > 100`\n",
    "- `annual_income = -1`\n",
    "- `zip_code = -1`\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/List_of_the_verified_oldest_people), the oldest living person is 115, as of 2018. I think it is reasonable to assume that any `age_of_driver > 100` in this dataset is a clerical error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, val_df, test_df]:\n",
    "    df.loc[df[\"age_of_driver\"] > 100, \"age_of_driver\"] = np.nan\n",
    "    df.loc[df[\"annual_income\"] == -1, \"annual_income\"] = np.nan\n",
    "    df.loc[df[\"zip_code\"] == 0, \"zip_code\"] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do an imputation for the missing values. Since there is only a very small percentage of missing values, we will simply do a mean/mode imputation for the continuous/categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, val_df, test_df]:\n",
    "    # mean imputation for continuous variables\n",
    "    for feature in [\"age_of_driver\", \"annual_income\", \"claim_est_payout\", \"age_of_vehicle\"]:\n",
    "        feature_mean = df.loc[:, feature].mean(skipna=True)\n",
    "        df[feature].fillna(int(feature_mean), inplace=True)\n",
    "\n",
    "    # mode imputation for categorical variables\n",
    "    for feature in [\"marital_status\", \"witness_present_ind\", \"zip_code\"]:\n",
    "        feature_mode = df.loc[:, feature].mode(dropna=True)\n",
    "        df[feature].fillna(feature_mode.values[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `claim_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, val_df, test_df]:\n",
    "    df.drop(columns=[\"claim_date\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some numerical features are measured in very different scales, so they should be re-scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\n",
    "    \"age_of_driver\", \"annual_income\", \"safty_rating\", \"past_num_of_claims\", \n",
    "    \"liab_prct\", \"claim_est_payout\", \"age_of_vehicle\", \"vehicle_price\", \"vehicle_weight\"\n",
    "]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_df[numerical_features])\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df[numerical_features] = scaler.transform(df[numerical_features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many unique `zip_code`. Creating dummy variables for `zip_code` will increase the dimensionality of the data too much. One idea is to transform it into `latitude` and `longitude` using the data from [UnitedStatesZipCodes.org](https://www.unitedstateszipcodes.org/zip-code-database/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_code_database = pd.read_csv(\"../data/external/zip_code_database.csv\")\n",
    "latitude_and_longitude_lookup = {\n",
    "    row.zip: (row.latitude, row.longitude) for row in zip_code_database.itertuples()\n",
    "}\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df[\"latitude\"] = df[\"zip_code\"].apply(lambda x: latitude_and_longitude_lookup[x][0])\n",
    "    df[\"longitude\"] = df[\"zip_code\"].apply(lambda x: latitude_and_longitude_lookup[x][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another idea is to use [target encoding](https://maxhalford.github.io/blog/target-encoding/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kingyiusuen/dev/travelers-insurance-fraud/venv/lib/python3.8/site-packages/category_encoders/target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/Users/kingyiusuen/dev/travelers-insurance-fraud/venv/lib/python3.8/site-packages/category_encoders/target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    }
   ],
   "source": [
    "target_encoder = TargetEncoder(cols=[\"zip_code\"])\n",
    "target_encoder.fit(train_df[\"zip_code\"], train_df[\"fraud\"])\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df[\"zip_code_target_encoded\"] = target_encoder.transform(df[\"zip_code\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop `zip_code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, val_df, test_df]:\n",
    "    df.drop(columns=[\"zip_code\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dummy variables for the other the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../data/processed/train.csv\", index=False)\n",
    "val_df.to_csv(\"../data/processed/val.csv\", index=False)\n",
    "test_df.to_csv(\"../data/processed/test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03e93f2959c516196957ae17ec0aa5d1e9fc5dd82cbe13968d4cfc2a60558992"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
