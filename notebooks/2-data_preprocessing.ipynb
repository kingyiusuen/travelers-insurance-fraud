{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This file shows how I performed data cleaning and feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full = pd.read_csv(\"../data/raw/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/raw/test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the test set provided does not have the target variable, so we have to create an internal validation set to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val = train_test_split(df_train_full, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the observations whose the target variable `fraud` is equal to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train[\"fraud\"] != -1]\n",
    "df_val = df_val[df_val[\"fraud\"] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For values that match the following conditions, treat them as missing values to be imputed later.\n",
    "\n",
    "- `age_of_driver > 100`\n",
    "- `annual_income = -1`\n",
    "- `zip_code = -1`\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/List_of_the_verified_oldest_people), the oldest living person is 115, as of 2018. I think it is reasonable to assume that any `age_of_driver > 100` in this dataset is a clerical error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_val, df_test]:\n",
    "    df.loc[df[\"age_of_driver\"] > 100, \"age_of_driver\"] = np.nan\n",
    "    df.loc[df[\"annual_income\"] == -1, \"annual_income\"] = np.nan\n",
    "    df.loc[df[\"zip_code\"] == 0, \"zip_code\"] = np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do an imputation for the missing values. Since there is only a very small percentage of missing values, we will simply do a mean/mode imputation for the continuous/categorical variables. Note that the mean/mode is computed based on the training set only to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_val, df_test]:\n",
    "    # mean imputation for continuous variables\n",
    "    for feature in [\"age_of_driver\", \"annual_income\", \"claim_est_payout\", \"age_of_vehicle\"]:\n",
    "        feature_mean = df_train.loc[:, feature].mean(skipna=True)\n",
    "        df[feature].fillna(int(feature_mean), inplace=True)\n",
    "\n",
    "    # mode imputation for categorical variables\n",
    "    for feature in [\"marital_status\", \"witness_present_ind\", \"zip_code\"]:\n",
    "        feature_mode = df_train.loc[:, feature].mode(dropna=True)\n",
    "        df[feature].fillna(feature_mode.values[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove features that do not seem to be related to the target variable (based on common sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_val, df_test]:\n",
    "    df.drop(columns=[\"claim_date\", \"claim_day_of_week\", \"vehicle_color\"], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many unique `zip_code`. Creating dummy variables for `zip_code` will increase the dimensionality of the data too much. One idea is to transform it into `latitude` and `longitude` using the data from [UnitedStatesZipCodes.org](https://www.unitedstateszipcodes.org/zip-code-database/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_code_database = pd.read_csv(\"../data/external/zip_code_database.csv\")\n",
    "latitude_and_longitude_lookup = {\n",
    "    row.zip: (row.latitude, row.longitude) for row in zip_code_database.itertuples()\n",
    "}\n",
    "\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    df[\"latitude\"] = df[\"zip_code\"].apply(lambda x: latitude_and_longitude_lookup[x][0])\n",
    "    df[\"longitude\"] = df[\"zip_code\"].apply(lambda x: latitude_and_longitude_lookup[x][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another idea is to use [target encoding](https://maxhalford.github.io/blog/target-encoding/), but after a few experiments it seems to perform worse than just transforming it to `latitude` and `longitude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from category_encoders.target_encoder import TargetEncoder\n",
    "#\n",
    "#target_encoder = TargetEncoder(cols=[\"zip_code\"], smoothing=10)\n",
    "#target_encoder.fit(df_train[\"zip_code\"], df_train[\"fraud\"])\n",
    "#\n",
    "#for df in [df_train, df_val, df_test]:\n",
    "#    df[\"zip_code_target_encoded\"] = target_encoder.transform(df[\"zip_code\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can drop `zip_code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_val, df_test]:\n",
    "    df.drop(columns=[\"zip_code\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"../data/processed/train.csv\", index=False)\n",
    "df_val.to_csv(\"../data/processed/val.csv\", index=False)\n",
    "df_test.to_csv(\"../data/processed/test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03e93f2959c516196957ae17ec0aa5d1e9fc5dd82cbe13968d4cfc2a60558992"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 02:22:02) \n[Clang 10.0.0 ]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
