{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "The models we tried include random forest, Adaboost, XGBoost and LightGBM. We will skip the steps of hyperparameter tuning and model selection, and present the final model we chose, which was a weighted average of XGBoost and LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_clean.csv')\n",
    "test = pd.read_csv('../data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {\n",
    "    'boosting_type': 'gbdt',  \n",
    "    'objective': 'binary', \n",
    "    'num_boost_round': 800,\n",
    "    'feature_fraction': .321, \n",
    "    'bagging_fraction': 0.50, \n",
    "    'min_child_samples': 100,  \n",
    "    'min_child_weigh': 35, \n",
    "    'max_depth': 3, \n",
    "    'num_leaves': 2, \n",
    "    'learing_rate': 0.15,\n",
    "    'reg_alpha': 5,\n",
    "    'reg_lambda': 1.1,\n",
    "    'metric':'auc',\n",
    "    'max_bin': 52,\n",
    "    'colsample_bytree': 0.9, \n",
    "    'subsample': 0.8, \n",
    "    'is_unbalance': 'true'\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 160,\n",
    "    'silent': True,\n",
    "    'objective': 'binary:logistic',\n",
    "    'gamma': 0.3,\n",
    "    'min_child_weight': 5,\n",
    "    'max_delta_step': 0,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.785,\n",
    "    'colsample_bylevel': 1,\n",
    "    'reg_alpha': 0.01,\n",
    "    'reg_lambda': 1,\n",
    "    'scale_pos_weight': 1,\n",
    "    'seed': 1440,\n",
    "    'missing': None\n",
    "}\n",
    "\n",
    "lgbm = LGBMClassifier(**lgbm_params)\n",
    "xgb = XGBClassifier(**xgb_params)\n",
    "avg_mod = VotingClassifier(\n",
    "    estimators=list(zip(['lgbm', 'xgb'], [lgbm, xgb])), \n",
    "    voting='soft', \n",
    "    weights=[6, 4]\n",
    ")\n",
    "\n",
    "avg_mod.fit(X_train.values, y_train.values)\n",
    "y_preds = avg_mod.predict_proba(X_test.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
